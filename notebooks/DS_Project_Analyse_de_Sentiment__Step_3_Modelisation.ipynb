{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7a283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Chargement du DataFrame depuis le fichier CSV\n",
    "df = pd.read_csv(\"../csv/scraping_commentaires_4_banques_nettoye.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd09e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(\"Shape of DataFrame:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les valeurs manquantes dans le DataFrame df et les supprimer\n",
    "missing_values = df.isnull().sum()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de librairies\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e06b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et la variable cible\n",
    "X = df[['Avis_nettoyé', 'Longueur_commentaire', 'Nombre_ponctuations', 'Nombre_points_exclamation', 'Polarite']]\n",
    "y = df['Note']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test (80% pour l'entraînement, 20% pour le test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde3df5",
   "metadata": {},
   "source": [
    "# StandardScaler / CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4ad0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Séparation des colonnes numériques de la colonne textuelle\n",
    "numeric_features = ['Longueur_commentaire', 'Nombre_ponctuations', 'Nombre_points_exclamation', 'Polarite']\n",
    "text_feature = 'Avis_nettoyé'\n",
    "\n",
    "# Instanciation d'un StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Utilisation du StandardScaler sur les colonnes numériques de X_train, et Conversion du résultat en DataFrame\n",
    "numeric_features_train_array = scaler.fit_transform(X_train[numeric_features])\n",
    "numeric_features_train_scaled = pd.DataFrame(numeric_features_train_array,\n",
    "                                             columns = numeric_features)\n",
    "\n",
    "# Utilisation du StandardScaler sur les colonnes numériques de X_test, et Conversion du résultat en DataFrame\n",
    "numeric_features_test_array = scaler.transform(X_test[numeric_features])\n",
    "numeric_features_test_scaled = pd.DataFrame(numeric_features_test_array,\n",
    "                                            columns = numeric_features)\n",
    "\n",
    "# Instanciation d'un CountVectorizer\n",
    "count_vectorizer = CountVectorizer(min_df = 50)\n",
    "\n",
    "# Utilisation du CountVectorizer sur la colonne textuelle de X_train, et Conversion du résultat en DataFrame\n",
    "text_feature_train_matrix = count_vectorizer.fit_transform(X_train[text_feature])\n",
    "text_feature_train_count = pd.DataFrame(text_feature_train_matrix.toarray(),\n",
    "                                        columns = count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Utilisation du CountVectorizer sur la colonne textuelle de X_test, et Conversion du résultat en DataFrame\n",
    "text_feature_test_matrix = count_vectorizer.transform(X_test[text_feature])\n",
    "text_feature_test_count = pd.DataFrame(text_feature_test_matrix.toarray(),\n",
    "                                       columns = count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Jointure des données transformées de X_train\n",
    "X_train_scaled_count = numeric_features_train_scaled.join(text_feature_train_count)\n",
    "\n",
    "# Jointure des données transformées de X_test\n",
    "X_test_scaled_count = numeric_features_test_scaled.join(text_feature_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f884e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"L'argument \\\"min_df = [valeur]\\\" dans CountVectorizer() permet de passer \\\n",
    "de 18.321 colonnes à {X_train_scaled_count.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2d013",
   "metadata": {},
   "source": [
    "## RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Instanciation du RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Application du RandomOverSampler sur X_train_scaled_count_pca et y_train\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train_scaled_count, y_train)\n",
    "\n",
    "# Affichage de la distribution des classes après l'application de RandomOverSampler\n",
    "print(\"Distribution des classes après l'application de RandomOverSampler:\", Counter(y_train_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f5d01",
   "metadata": {},
   "source": [
    "## Tester les modeles avec (RandomOverSampler + StandardScaler + CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e372f",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f122932",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparamètres pour la GridSearchCV de la LogisticRegression\n",
    "hyperparameters_LR1 = {\"C\" : [0.1, 1, 10],\n",
    "                       \"penalty\" : [\"l2\", \"l1\"],   # \"elasticnet\"\n",
    "                       \"solver\" : [\"liblinear\"]}\n",
    "# Autres hyperparamètres : solver = \"lbfgs\", class_weight = \"balanced\", max_iter = 1000, tol, random_state, ...\n",
    "\n",
    "# Instanciation d'une GridSearchCV pour la LogisticRegression\n",
    "clf_LR1 = LogisticRegression()\n",
    "grid_LR1 = GridSearchCV(estimator = clf_LR1, param_grid = hyperparameters_LR1, cv = 2)\n",
    "grid_LR1.fit(X_train_ros, y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1543697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"\\033[1mRésultats de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "# display(pd.DataFrame(grid_LR1.cv_results_))\n",
    "\n",
    "print(\"\\033[1mMeilleurs hyperparamètres de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "print(grid_LR1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a317e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction avec la GridSearchCV pour la LogisticRegression\n",
    "y_pred_LR1 = grid_LR1.predict(X_test_scaled_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_LR1))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_LR1, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e193f4",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d02256",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Classification SVC\n",
    "clf_SVC1 = SVC(gamma = 'scale')\n",
    "clf_SVC1.fit(X_train_ros, y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeea057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du SVC\n",
    "y_pred_SVC1 = clf_SVC1.predict(X_test_scaled_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de SVC :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_SVC1))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de SVC :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_SVC1, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ced2c",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7360f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade numpy scikit-learn threadpoolctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921148ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparamètres pour la GridSearchCV du KNeighborsClassifier\n",
    "hyperparameters_KNN1 = {\"n_neighbors\" : [3, 5, 7],\n",
    "                        \"metric\" : [\"minkowski\", \"manhattan\", \"chebyshev\", \"euclidean\"]}\n",
    "# Autres hyperparamètres : weights = \"uniform\" ou \"distance\", ...\n",
    "\n",
    "# Instanciation d'une GridSearchCV du KNeighborsClassifier\n",
    "clf_KNN1 = KNeighborsClassifier()\n",
    "grid_KNN1 = GridSearchCV(estimator = clf_KNN1, param_grid = hyperparameters_KNN1, cv = 2)\n",
    "grid_KNN1.fit(X_train_ros, y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\033[1mRésultats de la GridSearchCV du KNeighborsClassifier :\\033[0m \\n\")\n",
    "# display(pd.DataFrame(grid_KNN1.cv_results_))\n",
    "\n",
    "print(\"\\033[1mMeilleurs hyperparamètres de la GridSearchCV du KNeighborsClassifier :\\033[0m \\n\")\n",
    "print(grid_KNN1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction avec la GridSearchCV du KNeighborsClassifier\n",
    "y_pred_KNN1 = grid_KNN1.predict(X_test_scaled_count.values)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV du KNeighborsClassifier :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_KNN1))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV du KNeighborsClassifier :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_KNN1, rownames=['Classes réelles'], colnames=['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a97d9",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e401bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparamètres pour la GridSearchCV du DecisionTreeClassifier\n",
    "hyperparameters_DT1 = {\"criterion\" : [\"gini\", \"entropy\"],\n",
    "                       \"max_depth\" : [None, 3, 5, 7]}\n",
    "\n",
    "# Instanciation d'une GridSearchCV du DecisionTreeClassifier\n",
    "clf_DT1 = DecisionTreeClassifier()\n",
    "grid_DT1 = GridSearchCV(estimator = clf_DT1, param_grid = hyperparameters_DT1, cv = 2)\n",
    "grid_DT1.fit(X_train_ros, y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1mMeilleurs hyperparamètres de la GridSearchCV du DecisionTreeClassifier :\\033[0m \\n\")\n",
    "print(grid_DT1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0150fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction avec la GridSearchCV du DecisionTreeClassifier\n",
    "y_pred_DT1 = grid_DT1.predict(X_test_scaled_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV du DecisionTreeClassifier :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_DT1))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV du DecisionTreeClassifier :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_DT1, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a8564",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GB1 = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.5, max_depth = 2)\n",
    "clf_GB1.fit(X_train_ros, y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du GradientBoostingClassifier\n",
    "y_pred_GB1 = clf_GB1.predict(X_test_scaled_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV du GradientBoostingClassifier :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_GB1))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV du GradientBoostingClassifier :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_GB1, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728c1e1",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Instanciation du RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Application du RandomOverSampler sur X_train_scaled_count_pca et y_train\n",
    "X_train_ros_NB, y_train_ros_NB = ros.fit_resample(text_feature_train_count, y_train)\n",
    "\n",
    "# Affichage de la distribution des classes après l'application de RandomOverSampler\n",
    "print(\"Distribution des classes après l'application de RandomOverSampler:\", Counter(y_train_ros))\n",
    "\n",
    "clf_NB1 = MultinomialNB()\n",
    "clf_NB1.fit(X_train_ros_NB, y_train_ros_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du MultinomialNB\n",
    "y_pred_NB1 = clf_NB1.predict(text_feature_test_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV du MultinomialNB :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_NB1))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV du MultinomialNB :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_NB1, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9ec2a",
   "metadata": {},
   "source": [
    "## RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8db4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Instanciation du RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# Application du RandomUnderSampler sur X_train_scaled_count_pca et y_train\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train_scaled_count, y_train)\n",
    "\n",
    "# Affichage de la distribution des classes après l'application de RandomUnderSampler\n",
    "print(\"Distribution des classes après l'application de RandomUnderSampler:\", Counter(y_train_rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fa8a6",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparamètres pour la GridSearchCV de la LogisticRegression\n",
    "hyperparameters_LR2 = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\", \"l1\"],\n",
    "    \"solver\": [\"liblinear\"]\n",
    "}\n",
    "\n",
    "# Instanciation d'une GridSearchCV pour la LogisticRegression\n",
    "clf_LR2 = LogisticRegression()\n",
    "grid_LR2 = GridSearchCV(estimator=clf_LR2, param_grid=hyperparameters_LR2, cv=2)\n",
    "grid_LR2.fit(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814dedfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prédiction avec la GridSearchCV pour la LogisticRegression\n",
    "y_pred_LR2 = grid_LR2.predict(X_test_scaled_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_LR2))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_LR2, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fbd8e",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Classification SVC\n",
    "clf_SVC2 = SVC(gamma = 'scale')\n",
    "clf_SVC2.fit(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4636bcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prédiction du SVC\n",
    "y_pred_SVC2 = clf_SVC2.predict(X_test_scaled_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de SVC :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_SVC2))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de SVC :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_SVC2, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1c7c7",
   "metadata": {},
   "source": [
    "## SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Instanciation de SMOTE sans spécifier les paramètres\n",
    "smote = SMOTE()\n",
    "\n",
    "# Application de SMOTE sur X_train_scaled_count_pca\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled_count, y_train)\n",
    "\n",
    "# Affichage de la distribution des classes après l'application de SMOTE\n",
    "print(\"Distribution des classes après l'application de SMOTE:\", Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparamètres pour la GridSearchCV de la LogisticRegression\n",
    "hyperparameters_LR3 = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\", \"l1\"],\n",
    "    \"solver\": [\"liblinear\"]\n",
    "}\n",
    "\n",
    "# Instanciation d'une GridSearchCV pour la LogisticRegression\n",
    "clf_LR3 = LogisticRegression()\n",
    "grid_LR3 = GridSearchCV(estimator=clf_LR3, param_grid=hyperparameters_LR3, cv=2)\n",
    "grid_LR3.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678026fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction avec la GridSearchCV pour la LogisticRegression\n",
    "y_pred_LR3 = grid_LR3.predict(X_test_scaled_count)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_LR3))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_LR3, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9e066a7",
   "metadata": {},
   "source": [
    "Nous pouvons déjà constater à cette étape que l'oversampling, l'undersampling et SMOTE ne nous donnent pas des résultats satisfaisants avec tous les modèles testés. Nous pensons qu'il serait plus pertinent de s'arrêter ici et de résoudre le problème du déséquilibre des données. Nous pourrions envisager de regrouper la variable cible pour réduire le nombre de classes et équilibrer ainsi la distribution des données. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da7793",
   "metadata": {},
   "source": [
    "# Regrouper la variable cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_distribution = df['Note'].value_counts().sort_index()\n",
    "print(note_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des catégories\n",
    "def categorize_rating(rating):\n",
    "    if rating in [4, 5]:\n",
    "        return 1  # Positif\n",
    "    else:\n",
    "        return 0  # Négatif\n",
    "\n",
    "# Appliquer la fonction de catégorisation à la colonne des notes\n",
    "df['Note_cat'] = df['Note'].apply(categorize_rating)\n",
    "\n",
    "# Vérifier la distribution des nouvelles catégories\n",
    "note_cat_distribution = df['Note_cat'].value_counts()\n",
    "print(note_cat_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et la variable cible\n",
    "X = df[['Avis_nettoyé', 'Longueur_commentaire', 'Nombre_ponctuations', 'Nombre_points_exclamation', 'Polarite']]\n",
    "y = df['Note_cat']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test (80% pour l'entraînement, 20% pour le test)\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X, y, test_size=0.2, shuffle = True, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38431d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des colonnes numériques de la colonne textuelle\n",
    "numeric_features_cat = ['Longueur_commentaire', 'Nombre_ponctuations', 'Nombre_points_exclamation', 'Polarite']\n",
    "text_feature_cat = 'Avis_nettoyé'\n",
    "\n",
    "# Instanciation d'un StandardScaler\n",
    "scaler_cat = StandardScaler()\n",
    "\n",
    "# Utilisation du StandardScaler sur les colonnes numériques de X_train_cat, et Conversion du résultat en DataFrame\n",
    "numeric_features_train_array_cat = scaler_cat.fit_transform(X_train_cat[numeric_features_cat])\n",
    "numeric_features_train_scaled_cat = pd.DataFrame(numeric_features_train_array_cat,\n",
    "                                             columns = [f\"{feature}_cat\" for feature in numeric_features_cat])\n",
    "\n",
    "# Utilisation du StandardScaler sur les colonnes numériques de X_test_cat, et Conversion du résultat en DataFrame\n",
    "numeric_features_test_array_cat = scaler_cat.transform(X_test_cat[numeric_features_cat])\n",
    "numeric_features_test_scaled_cat = pd.DataFrame(numeric_features_test_array_cat,\n",
    "                                            columns = [f\"{feature}_cat\" for feature in numeric_features_cat])\n",
    "\n",
    "# Instanciation d'un CountVectorizer\n",
    "count_vectorizer_cat = CountVectorizer(min_df = 50)\n",
    "\n",
    "# Utilisation du CountVectorizer sur la colonne textuelle de X_train_cat, et Conversion du résultat en DataFrame\n",
    "text_feature_train_matrix_cat = count_vectorizer_cat.fit_transform(X_train_cat[text_feature_cat])\n",
    "text_feature_train_count_cat = pd.DataFrame(text_feature_train_matrix_cat.toarray(),\n",
    "                                        columns = count_vectorizer_cat.get_feature_names_out())\n",
    "\n",
    "# Utilisation du CountVectorizer sur la colonne textuelle de X_test_cat, et Conversion du résultat en DataFrame\n",
    "text_feature_test_matrix_cat = count_vectorizer_cat.transform(X_test_cat[text_feature_cat])\n",
    "text_feature_test_count_cat = pd.DataFrame(text_feature_test_matrix_cat.toarray(),\n",
    "                                       columns = count_vectorizer_cat.get_feature_names_out())\n",
    "\n",
    "# Jointure des données transformées de X_train_cat\n",
    "X_train_scaled_count_cat = numeric_features_train_scaled_cat.join(text_feature_train_count_cat)\n",
    "\n",
    "# Jointure des données transformées de X_test_cat\n",
    "X_test_scaled_count_cat = numeric_features_test_scaled_cat.join(text_feature_test_count_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"L'argument \\\"min_df = [valeur]\\\" dans CountVectorizer() permet de passer \\\n",
    "de 18.321 colonnes à {X_train_scaled_count_cat.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfe871",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparamètres pour la GridSearchCV de la LogisticRegression\n",
    "hyperparameters_LR = {\"C\" : [0.1, 1, 10],\n",
    "                       \"penalty\" : [\"l2\", \"l1\"],   # \"elasticnet\"\n",
    "                       \"solver\" : [\"liblinear\"]}\n",
    "\n",
    "# Instanciation d'une GridSearchCV pour la LogisticRegression\n",
    "clf_LR_cat = LogisticRegression()\n",
    "grid_LR_cat = GridSearchCV(estimator = clf_LR_cat, param_grid = hyperparameters_LR, cv = 2)\n",
    "grid_LR_cat.fit(X_train_scaled_count_cat, y_train_cat) # Prédiction avec la GridSearchCV pour la LogisticRegression\n",
    "\n",
    "y_pred_LR_cat = grid_LR_cat.predict(X_test_scaled_count_cat)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "print(classification_report(y_test_cat, y_pred_LR_cat))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV pour la LogisticRegression :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test_cat, y_pred_LR_cat, rownames=['Classes réelles'], colnames=['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3e64d",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Classification SVC\n",
    "clf_SVC_cat = SVC(gamma='scale')\n",
    "clf_SVC_cat.fit(X_train_scaled_count_cat, y_train_cat)  # Prédiction du SVC\n",
    "y_pred_SVC_cat = clf_SVC_cat.predict(X_test_scaled_count_cat)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de SVC :\\033[0m \\n\")\n",
    "print(classification_report(y_test_cat, y_pred_SVC_cat))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de SVC :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test_cat, y_pred_SVC_cat, rownames=['Classes réelles'], colnames=['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1db0a",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instanciation d'un KNeighborsClassifier\n",
    "clf_KNN_cat = KNeighborsClassifier()\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf_KNN_cat.fit(X_train_scaled_count_cat.values, y_train_cat.values)\n",
    "\n",
    "# Prédiction avec le modèle\n",
    "y_pred_KNN_cat = clf_KNN_cat.predict(X_test_scaled_count_cat.values)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(\"\\033[1mRapport de classification du KNeighborsClassifier :\\033[0m \\n\")\n",
    "print(classification_report(y_test_cat, y_pred_KNN_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498419f",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49144b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Hyperparamètres pour la GridSearchCV du DecisionTreeClassifier\n",
    "hyperparameters_DT_cat = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "                       \"max_depth\": [None, 3, 5, 7]}\n",
    "\n",
    "# Instanciation d'une GridSearchCV du DecisionTreeClassifier\n",
    "clf_DT_cat = DecisionTreeClassifier()\n",
    "grid_DT_cat = GridSearchCV(estimator=clf_DT_cat, param_grid=hyperparameters_DT_cat, cv=2)\n",
    "grid_DT_cat.fit(X_train_scaled_count_cat, y_train_cat)\n",
    "\n",
    "# Prédiction avec la GridSearchCV du DecisionTreeClassifier\n",
    "y_pred_DT_cat = grid_DT_cat.predict(X_test_scaled_count_cat)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV du DecisionTreeClassifier :\\033[0m \\n\")\n",
    "print(classification_report(y_test_cat, y_pred_DT_cat))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV du DecisionTreeClassifier :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test_cat, y_pred_DT_cat, rownames=['Classes réelles'], colnames=['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126f2b7",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GB_cat = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5, max_depth=2)\n",
    "clf_GB_cat.fit(X_train_scaled_count_cat, y_train_cat)\n",
    "\n",
    "# Prédiction du GradientBoostingClassifier\n",
    "y_pred_GB_cat = clf_GB_cat.predict(X_test_scaled_count_cat)\n",
    "\n",
    "print(\"\\033[1mRapport de classification de la GridSearchCV du GradientBoostingClassifier :\\033[0m \\n\")\n",
    "print(classification_report(y_test_cat, y_pred_GB_cat))\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion de la GridSearchCV du GradientBoostingClassifier :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test_cat, y_pred_GB_cat, rownames=['Classes réelles'], colnames=['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b23c4b",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcca9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#clf_NB1_cat = MultinomialNB()\n",
    "#clf_NB1_cat.fit(text_feature_train_count_cat, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du MultinomialNB\n",
    "#y_pred_NB1_cat = clf_NB1.predict(text_feature_test_count_cat)\n",
    "\n",
    "#print(\"\\033[1mRapport de classification de la GridSearchCV du MultinomialNB :\\033[0m \\n\")\n",
    "#print(classification_report(y_test_cat, y_pred_NB1_cat))\n",
    "\n",
    "#print(\"\\033[1mMatrice de confusion de la GridSearchCV du MultinomialNB :\\033[0m \\n\")\n",
    "#display(pd.crosstab(y_test_cat, y_pred_NB1_cat, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed73d7",
   "metadata": {},
   "source": [
    "## Analyse des coefficients : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des coefficients\n",
    "coefficients = grid_LR_cat.best_estimator_.coef_[0]\n",
    "feature_names = X_train_scaled_count_cat.columns\n",
    "\n",
    "# Création d'un DataFrame pour une meilleure visualisation\n",
    "coefficients_df = pd.DataFrame({'Variable': feature_names, 'Coefficient': coefficients})\n",
    "coefficients_df = coefficients_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Affichage des coefficients\n",
    "print(coefficients_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5035be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Diviser le DataFrame en deux parties : positifs et négatifs\n",
    "positive_coefficients = coefficients_df[coefficients_df['Coefficient'] > 0].tail(10)\n",
    "negative_coefficients = coefficients_df[coefficients_df['Coefficient'] < 0].head(10)\n",
    "\n",
    "# Concaténer les deux parties\n",
    "top_coefficients = pd.concat([positive_coefficients, negative_coefficients])\n",
    "\n",
    "# Inverser l'ordre des lignes dans le DataFrame\n",
    "top_coefficients = top_coefficients.iloc[::-1]\n",
    "\n",
    "# Créer le graphique\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red' if c < 0 else 'blue' for c in top_coefficients['Coefficient']]\n",
    "plt.barh(top_coefficients['Variable'], top_coefficients['Coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Top 10 des Coefficients de la Régression Logistique')\n",
    "plt.axvline(x=0, color='black', linewidth=0.5)\n",
    "for i, v in enumerate(top_coefficients['Coefficient']):\n",
    "    plt.text(v, i, str(round(v, 2)), color='black', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39219a14",
   "metadata": {},
   "source": [
    "## Deep Learning \"Note\" de 1 à 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8afa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (X_train_scaled_count.shape[1],), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "dense3 = Dense(units = 6, activation = \"tanh\", name = \"Dense_3\")\n",
    "dense4 = Dense(units = 5, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "x = dense1(inputs)\n",
    "x = dense2(x)\n",
    "x = dense3(x)\n",
    "outputs = dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "y_train_dl = y_train - 1\n",
    "\n",
    "model.fit(X_train_scaled_count, y_train_dl, epochs = 40, batch_size = 512, validation_split = 0.2)\n",
    "\n",
    "#Prédiction du model\n",
    "y_pred_prob = model.predict(X_test_scaled_count)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis = 1) + 1\n",
    "\n",
    "print(\"\\033[1mRapport de classification du model :\\033[0m \\n\")\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion du model :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test, y_pred_class, rownames = ['Classes réelles'], colnames = ['Classes prédites'])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cde958",
   "metadata": {},
   "source": [
    "## Deep Learning \"Note\" 2 classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbd2c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "inputs = Input(shape=(X_train_scaled_count_cat.shape[1],), name=\"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "dense3 = Dense(units = 6, activation = \"tanh\", name = \"Dense_3\")\n",
    "dense4 = Dense(units=1, activation=\"sigmoid\", name=\"Dense_4\")\n",
    "\n",
    "x = dense1(inputs)\n",
    "x = dense2(x)\n",
    "x = dense3(x)\n",
    "outputs = dense4(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled_count_cat, y_train_cat, epochs=40, batch_size=512, validation_split=0.2)\n",
    "\n",
    "# Prediction\n",
    "y_pred_prob_cat = model.predict(X_test_scaled_count_cat)\n",
    "y_pred_class_cat = (y_pred_prob_cat > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Flatten y_pred_class_cat\n",
    "y_pred_class_cat = y_pred_class_cat.flatten()\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\033[1mClassification Report:\\033[0m \\n\")\n",
    "print(classification_report(y_test_cat, y_pred_class_cat))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1mMatrice de confusion du model :\\033[0m \\n\")\n",
    "display(pd.crosstab(y_test_cat, y_pred_class_cat, rownames = ['Classes réelles'], colnames = ['Classes prédites']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
