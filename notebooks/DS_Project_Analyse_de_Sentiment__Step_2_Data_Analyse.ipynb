{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des packages necessaires à l'analyse\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79fd9b",
   "metadata": {},
   "source": [
    "# Exploration de donnes et Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import du DataFrame et affichage des premières lignes\n",
    "\n",
    "filepath = \"../csv/scraping_commentaires_4_banques.csv\"\n",
    "df=pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Informations du DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe7d98a4",
   "metadata": {},
   "source": [
    "Pour notre première Data Visualization (sans nettoyer les données), nous avons utilisé la méthode de la Wordcloud pour visualiser les mots les plus fréquents dans les commentaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires pour NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Créer une copie des données pour la manipulation\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Concaténer tous les avis en une seule chaîne de caractères\n",
    "all_comments = ' '.join(df_copy['Avis'].dropna())\n",
    "\n",
    "# Tokenisation des commentaires\n",
    "tokens = word_tokenize(all_comments)\n",
    "\n",
    "# Suppression des stopwords\n",
    "stop_words = set(stopwords.words('french')) # stopwords en français\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Création de la Wordcloud avec des options personnalisées\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis',\n",
    "                      contour_color='steelblue', contour_width=2,\n",
    "                      font_path=None).generate(' '.join(filtered_tokens))\n",
    "\n",
    "# Affichage de la Wordcloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Nuage de mots des commentaires')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf1bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le nettoyage et le traitement des données:\n",
    "#Supprimer les doublons\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Supprimer les valeurs manquantes\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37725bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer le préfixe \"Date de l'expérience: \" de la colonne \"Date de l'expérience\"\n",
    "df['Date de l\\'expérience'] = df['Date de l\\'expérience'].str.replace('Date de l\\'expérience: ', '')\n",
    "\n",
    "# Convertir la colonne \"Date de publication\" en objets datetime\n",
    "df['Date de publication'] = pd.to_datetime(df['Date de publication'])\n",
    "\n",
    "# Créer trois nouvelles colonnes pour le jour, le mois et l'année de publication\n",
    "df['DayPub'] = df['Date de publication'].dt.day\n",
    "df['MonthPub'] = df['Date de publication'].dt.month\n",
    "df['YearPub'] = df['Date de publication'].dt.year\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation et Lemmatization du DataFrame puis réaffichage du nuage de mots\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Suppression des caractères spéciaux et de la ponctuation\n",
    "    processed_comments = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # Conversion en minuscules\n",
    "    processed_comments = processed_comments.lower()\n",
    "    \n",
    "    # Tokenisation du texte\n",
    "    tokens = word_tokenize(processed_comments)\n",
    "    \n",
    "    # Suppression des mots vides\n",
    "    french_stopwords = set(stopwords.words(\"french\"))\n",
    "    custom_stopwords = [\"boursorama\", \"monabanq\", \"hello\", \"bank\", \"fortuneo\", \"ça\", \"cette\", \"ca\", \"c'est\", \"en\", \n",
    "                    \"aussi\", \"ni\", \"tout\", \"déjà\", \"pour\", \"de\", \"et\", \"à\", \"le\", \"je\", \"la\", \"un\", \"pas\", \"que\",\n",
    "                    \"les\", \"des\", \"une\", \"très\", \"si\", \"leurs\", \"aprè\", \"toute\", \"quand\", \"non\", \"peu\", \"jai\", \n",
    "                    \"cela\", \"ca\", \"cest\", \"a\", \"chez\", \"banque\", \"car\", \"donc\"]\n",
    "    french_stopwords.update(custom_stopwords)\n",
    "    tokens = [word for word in tokens if word not in french_stopwords]\n",
    "    \n",
    "    # Lemmatisation des tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Joindre les tokens en une seule chaine\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce743bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier les catégories de la variable cible\n",
    "sorted_categories = df['Note'].value_counts().sort_index()\n",
    "\n",
    "# Tracer le graphique avec les catégories triées\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Note', data=df, order=sorted_categories.index)\n",
    "plt.title('Distribution de la variable cible (Note)')\n",
    "plt.xlabel('Note')\n",
    "plt.ylabel(\"Nombre d'échantillons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa74663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la bibliothèque locale\n",
    "import locale\n",
    "# Définir la localisation en français\n",
    "locale.setlocale(locale.LC_TIME, 'fr_FR.UTF-8')\n",
    "\n",
    "# Extraire le jour de la semaine\n",
    "df['DayOfWeek'] = df['Date de publication'].dt.strftime('%A')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01917774",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Répartition des commentaires par jour de la semaine\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='DayOfWeek', data=df, order=['lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche'])\n",
    "plt.title('Répartition des commentaires par jour de la semaine')\n",
    "plt.xlabel('Jour de la semaine')\n",
    "plt.ylabel('Nombre de commentaires')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Répartition des commentaires par mois de l'année\n",
    "mois_fr = ['janvier', 'février', 'mars', 'avril', 'mai', 'juin', 'juillet', 'août', 'septembre', 'octobre', 'novembre', 'décembre']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = df['MonthPub'].value_counts().sort_index().plot(kind='bar')\n",
    "ax.set_title('Répartition des commentaires par mois de l\\'année')\n",
    "ax.set_xlabel('Mois')\n",
    "ax.set_ylabel('Nombre de commentaires')\n",
    "ax.set_xticklabels(mois_fr, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87853300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Répartition des commentaires par l'année et par banque à partir de 2017\n",
    "# Filtrer les données à partir de 2017\n",
    "data_2017 = df[df['YearPub'] >= 2017]\n",
    "\n",
    "# Grouper les données par année et par banque\n",
    "grouped_data = data_2017.groupby(['YearPub', 'Banque']).size().unstack(fill_value=0)\n",
    "\n",
    "# Afficher le graphique à barres empilées\n",
    "grouped_data.plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "plt.title('Répartition des commentaires par année et par banque à partir de 2017')\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Nombre de commentaires')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Banque', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4240f6",
   "metadata": {},
   "source": [
    "# Feature engineering et Pre-processing "
   ]
  },
  {
   "cell_type": "raw",
   "id": "dad165e5",
   "metadata": {},
   "source": [
    "Creation de nouvelles variables - Longueur_commentaire, Nombre_ponctuations et Nombre_points_exclamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ce672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Longueur de la réponse\n",
    "df['Longueur_commentaire'] = df['Avis'].apply(lambda x: len(x))\n",
    "\n",
    "# Nombre de ponctuations\n",
    "def count_punctuation(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return count\n",
    "\n",
    "df['Nombre_ponctuations'] = df['Avis'].apply(lambda x: count_punctuation(x))\n",
    "\n",
    "# Focalisation sur le nombre de points d'exclamation\n",
    "def count_exclamation(text):\n",
    "    count = text.count('!')\n",
    "    return count\n",
    "\n",
    "df['Nombre_points_exclamation'] = df['Avis'].apply(lambda x: x.count('!'))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Effectuer le test ANOVA\n",
    "model = sm.OLS.from_formula('Longueur_commentaire ~ Note', data=df).fit()\n",
    "# Afficher les résultats de régression\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82823bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier les données par la colonne 'Note'\n",
    "df_sorted = df.sort_values(by='Note')\n",
    "\n",
    "# Tracer le boxen plot avec les données triées\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxenplot(x='Note', y='Longueur_commentaire', data=df_sorted)\n",
    "plt.title('Distribution de la longueur des commentaires en fonction de la note attribuée')\n",
    "plt.xlabel('Note attribuée')\n",
    "plt.ylabel('Longueur des commentaires')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c64b3e06",
   "metadata": {},
   "source": [
    "Analyser l'impact des variables \"Nombre_ponctuations\" et \"Nombre_points_exclamation\" sur l'avis positif ou négatif en effectuant une analyse exploratoire des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la colonne \"Note\" en type numérique\n",
    "df['Note'] = pd.to_numeric(df['Note'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "sns.boxplot(x='Note', y='Nombre_ponctuations', data=df)\n",
    "plt.title('Relation entre le nombre de ponctuations et la note du commentaire')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x='Note', y='Nombre_points_exclamation', data=df)\n",
    "plt.title('Relation entre le nombre de points d\\'exclamation et la note du commentaire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96dafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer le test ANOVA pour Nombre_ponctuations\n",
    "formula_ponctuations = 'Nombre_ponctuations ~ Note'\n",
    "model_ponctuations = sm.OLS.from_formula(formula_ponctuations, data=df).fit()\n",
    "print(\"Pour Nombre_ponctuations :\")\n",
    "print(model_ponctuations.summary())\n",
    "\n",
    "# Effectuer le test ANOVA pour Nombre_points_exclamation\n",
    "formula_exclamation = 'Nombre_points_exclamation ~ Note'\n",
    "model_exclamation = sm.OLS.from_formula(formula_exclamation, data=df).fit()\n",
    "print(\"\\nPour Nombre_points_exclamation :\")\n",
    "print(model_exclamation.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75d7af6e",
   "metadata": {},
   "source": [
    "Creation de nouvelles variables - Polarité, Subjectivité et Sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d6cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Appliquer le nettoyage du texte sur la colonne 'Avis'\n",
    "df['Avis_nettoyé'] = df['Avis'].apply(clean_text)\n",
    "\n",
    "# Créer la fonction pour calculer la polarité et la subjectivité en utilisant TextBlob avec le modèle de langue français\n",
    "def calculer_sentiment_polarite_subjectivite_francais(post):\n",
    "    blob = TextBlob(post, analyzer=PatternAnalyzer())\n",
    "    polarite = blob.sentiment[0]\n",
    "    subjectivite = blob.sentiment[1]\n",
    "    \n",
    "    # Déterminer le sentiment basé sur la polarité\n",
    "    if polarite > 0:\n",
    "        sentiment = \"Positif\"\n",
    "    elif polarite < 0:\n",
    "        sentiment = \"Négatif\"\n",
    "    else:\n",
    "        sentiment = \"Neutre\"\n",
    "    \n",
    "    return sentiment, polarite, subjectivite\n",
    "\n",
    "# Appliquer la fonction calculer_sentiment_polarite_subjectivite_francais à la colonne 'Avis_nettoyé' de votre DataFrame\n",
    "df['Sentiment'], df['Polarite'], df['Subjectivite'] = zip(*df['Avis_nettoyé'].apply(calculer_sentiment_polarite_subjectivite_francais))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791cc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Polarite', 'Subjectivite']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c59da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "labels = sentiment_counts.index.tolist()\n",
    "values = sentiment_counts.values.tolist()\n",
    "\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%')\n",
    "plt.title(\"Répartition des sentiments\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23450ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir la colonne 'Date de publication' en objets datetime\n",
    "df['Date de publication'] = pd.to_datetime(df['Date de publication'])\n",
    "\n",
    "# Extraire uniquement la date\n",
    "df['Date'] = df['Date de publication'].dt.date\n",
    "\n",
    "# Afficher les premières lignes du DataFrame avec la nouvelle colonne 'Date'\n",
    "\n",
    "# Tri du DataFrame par ordre croissant des index\n",
    "df = df.sort_index(ascending=True)\n",
    "\n",
    "# Affichage du DataFrame trié\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8db3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliquer le nettoyage du texte sur la colonne 'Avis'\n",
    "df['Avis_nettoyé'] = df['Avis'].apply(clean_text)\n",
    "\n",
    "# Concaténer tous les avis nettoyés en une seule chaîne de caractères\n",
    "cleaned_comments = ' '.join(df['Avis_nettoyé'])\n",
    "\n",
    "# Tokenisation des commentaires nettoyés\n",
    "cleaned_tokens = word_tokenize(cleaned_comments)\n",
    "\n",
    "# Création de la Wordcloud avec des options personnalisées\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis',\n",
    "                      contour_color='steelblue', contour_width=2,\n",
    "                      font_path=None).generate(' '.join(cleaned_tokens))\n",
    "\n",
    "# Affichage de la Wordcloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Nuage de mots des commentaires après nettoyage et lemmatisation')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68c6bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "# Compter les occurrences de chaque mot dans les commentaires nettoyés\n",
    "word_counter = Counter(' '.join(df['Avis_nettoyé']).split())\n",
    "\n",
    "# Sélectionner les 15 mots les plus fréquents\n",
    "top_15_words = word_counter.most_common(15)\n",
    "\n",
    "# Extraire les mots et leurs fréquences\n",
    "mots = [mot[0] for mot in top_15_words]\n",
    "freq = [mot[1] for mot in top_15_words]\n",
    "\n",
    "# Créer un histogramme avec seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=mots, y=freq)\n",
    "plt.title('15 mots les plus fréquemment employés par les clients dans les commentaires')\n",
    "plt.xlabel('Mots')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définir une fonction pour générer le nuage de mots\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Grouper les avis par polarité\n",
    "avis_par_polarite = {\n",
    "    'Positif': df[df['Polarite'] > 0]['Avis_nettoyé'],\n",
    "    'Négatif': df[df['Polarite'] < 0]['Avis_nettoyé'],\n",
    "    'Neutre': df[df['Polarite'] == 0]['Avis_nettoyé']\n",
    "}\n",
    "\n",
    "# Générer les nuages de mots pour chaque type d'avis\n",
    "for polarite, avis in avis_par_polarite.items():\n",
    "    generate_wordcloud(avis, f'Nuage de mots - Avis {polarite.lower()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Compter les occurrences de chaque mot dans les commentaires nettoyés\n",
    "word_counter = Counter(' '.join(df['Avis_nettoyé']).split())\n",
    "\n",
    "# Division des avis en tokens en fonction des catégories\n",
    "tokens_positifs = [token for avis in df[df['Sentiment'] == 'Positif']['Avis_nettoyé'] for token in avis.split()]\n",
    "tokens_negatifs = [token for avis in df[df['Sentiment'] == 'Négatif']['Avis_nettoyé'] for token in avis.split()]\n",
    "tokens_neutres = [token for avis in df[df['Sentiment'] == 'Neutre']['Avis_nettoyé'] for token in avis.split()]\n",
    "\n",
    "from collections import Counter\n",
    "#Obtenir les mots les plus utilisés\n",
    "def get_maxtoken(avis, num=30):\n",
    "    word_tokens = Counter(avis)\n",
    "    max_common = word_tokens.most_common(num)\n",
    "    return dict(max_common)\n",
    "\n",
    "def token_df_vis(x,title):\n",
    "    df_token = pd.DataFrame(get_maxtoken(x).items(), columns=['words','count'])\n",
    "    fig = px.bar(df_token, x='words', y='count', title = title)\n",
    "    fig.show()\n",
    "\n",
    "# Créer les histogrammes pour chaque catégorie d'avis\n",
    "token_df_vis(tokens_positifs,'Positive')\n",
    "token_df_vis(tokens_negatifs,'Negative')\n",
    "token_df_vis(tokens_neutres,'Neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6812ba",
   "metadata": {},
   "source": [
    "# CountVectorizer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e08d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade --user scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sélectionner les colonnes pertinentes\n",
    "df_note = df[['Note', 'Avis_nettoyé']]\n",
    "\n",
    "# Appliquer CountVectorizer sur les avis nettoyés\n",
    "vectorizer = CountVectorizer()\n",
    "features = vectorizer.fit_transform(df_note['Avis_nettoyé'])\n",
    "\n",
    "# Exploration de la matrice de comptage\n",
    "print(\"Dimensions de la matrice de caractéristiques:\", features.shape)\n",
    "print(\"Vocabulaire:\", list(vectorizer.vocabulary_.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test (80% pour l'entraînement, 20% pour le test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df['Note'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Afficher les dimensions des ensembles d'entraînement et de test\n",
    "print(\"Dimensions de l'ensemble d'entraînement :\", X_train.shape, y_train.shape)\n",
    "print(\"Dimensions de l'ensemble de test :\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d86d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du modèle RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=-1) \n",
    "# Entraînement du modèle\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Affichage de la précision sur l'ensemble de test\n",
    "accuracy = rf.score(X_test, y_test)\n",
    "print(\"Précision sur l'ensemble de test :\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e885a4f",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Appliquer TF-IDFVectorizer sur les avis nettoyés\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "features_tfidf = tfidf_vectorizer.fit_transform(df_note['Avis_nettoyé'])\n",
    "\n",
    "# Exploration de la matrice TF-IDF\n",
    "print(\"Dimensions de la matrice de caractéristiques avec TF-IDFVectorizer:\", features_tfidf.shape)\n",
    "print(\"Vocabulaire:\", list(tfidf_vectorizer.vocabulary_.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, df['Note'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du modèle RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=-1) \n",
    "\n",
    "# Entraînement du modèle\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Affichage de la précision sur l'ensemble de test\n",
    "accuracy = rf.score(X_test, y_test)\n",
    "print(\"Précision sur l'ensemble de test avec TF-IDFVectorizer :\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caca5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = return_ngram(df['Avis_nettoyé'], ngram_range=(3, 3))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='count', y='word', data=count_words.head(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000be108",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_words = return_ngram(df['Avis_nettoyé'], ngram_range=(6, 6))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='count', y='word', data=count_words.head(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f36397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 20))\n",
    "\n",
    "for note in range(1, 6):\n",
    "    avis_note = df[df['Note'] == note]['Avis_nettoyé']\n",
    "    vectorizer = CountVectorizer()\n",
    "    features = vectorizer.fit_transform(avis_note)\n",
    "    word_counts = features.sum(axis=0)\n",
    "    word_names = vectorizer.get_feature_names_out()\n",
    "    word_count_dict = dict(zip(word_names, word_counts.A1))\n",
    "    \n",
    "    sorted_words = sorted(word_count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = [word for word, _ in sorted_words[:10]]\n",
    "    \n",
    "    row = (note - 1) // 2\n",
    "    col = (note - 1) % 2\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    ax.barh(top_words, [word_count_dict[word] for word in top_words])\n",
    "    ax.set_title(f\"Top 10 mots - Note {note}\")\n",
    "    ax.set_xlabel(\"Nombre d'occurrences\")\n",
    "    ax.set_ylabel(\"Mots\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029d9e3",
   "metadata": {},
   "source": [
    "# Annex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40976498",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ANALYSE PAR BANQUE\n",
    "banque_counts = df['Banque'].value_counts()\n",
    "banque_counts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "611b84c0",
   "metadata": {},
   "source": [
    "Cadre du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des commentaires par banque\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Banque', data=df)\n",
    "plt.title('Répartition des commentaires par banque')\n",
    "plt.xlabel('Banque')\n",
    "plt.ylabel('Nombre de commentaires')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Comparaison des notes attribuées aux différentes banques\n",
    "# Calculer la moyenne des notes attribuées à chaque banque\n",
    "mean_notes = df.groupby('Banque')['Note'].mean().sort_values()\n",
    "\n",
    "# Créer un graphique à barres pour visualiser les moyennes des notes attribuées aux différentes banques\n",
    "plt.figure(figsize=(10, 6))\n",
    "mean_notes.plot(kind='bar', color='skyblue')\n",
    "plt.title('Moyenne des notes attribuées aux différentes banques')\n",
    "plt.xlabel('Banque')\n",
    "plt.ylabel('Note moyenne')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
